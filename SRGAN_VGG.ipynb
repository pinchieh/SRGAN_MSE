{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "\n",
    "import string\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, scipy, multiprocessing\n",
    "import matplotlib.image as img\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "import cv2\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_hr_img_list = sorted(tl.files.load_file_list(path=\".\\\\image_2\", regx=\\'.*.png\\', printable=False))\\ntrain_hr_imgs = tl.vis.read_images(train_hr_img_list, path=\".\\\\image_2\", n_threads=32)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_hr_img_list = sorted(tl.files.load_file_list(path=\".\\image_2\", regx='.*.png', printable=False))\n",
    "train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=\".\\image_2\", n_threads=32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train():\n",
    "        for img in train_hr_imgs:\n",
    "            yield img\n",
    "def _map_fn_train(img):\n",
    "        hr_patch = tf.image.random_crop(img, [256, 256, 3])\n",
    "        hr_patch = hr_patch / (255. / 2.)\n",
    "        hr_patch = hr_patch - 1.\n",
    "        hr_patch = tf.image.random_flip_left_right(hr_patch)\n",
    "        lr_patch = tf.image.resize_images(hr_patch, (64, 64), method=2)\n",
    "        #lr_patch = tf.image.resize(hr_patch, size=[64, 64])\n",
    "        return lr_patch, hr_patch\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = tf.data.Dataset.from_generator(generator_train, output_types=(tf.float32))\n",
    "train_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = train_ds.shuffle(3*9)\n",
    "train_ds = train_ds.batch(9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv2d(input_, output_shape,\n",
    "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "             name=\"deconv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        w = tf.get_variable('w', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "\n",
    "        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,strides=[1, d_h, d_w, 1])\n",
    "\n",
    "\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "        \n",
    "        return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, noise_z, training_phase, hparas, reuse):\n",
    "        self.z = noise_z\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        \"\"\" Generator in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n",
    "        feature maps (n) and stride (s) feature maps (n) and stride (s)\n",
    "        \"\"\"\n",
    "        relu = tf.nn.relu\n",
    "        tanh = tf.nn.tanh\n",
    "        conv2d = tf.layers.conv2d\n",
    "        bn = tf.layers.batch_normalization\n",
    "        linear = tf.layers.dense\n",
    "        with tf.variable_scope(\"generator\", reuse=self.reuse) as vs:\n",
    "            n = relu(conv2d(self.z, 64, kernel_size=3, strides=1, padding='same'))\n",
    "            temp = n\n",
    "            # B residual blocks\n",
    "            for i in range(16):\n",
    "                nn = conv2d(n, 64, kernel_size=3, strides=1, padding='same')\n",
    "                nn = relu(bn(nn,training = self.train ))\n",
    "                nn = conv2d(nn, 64, kernel_size=3, strides=1, padding='same')\n",
    "                nn = bn(nn,training = self.train )\n",
    "                nn = tf.add(n,nn)\n",
    "            n = conv2d(n, 64, kernel_size=3, strides=1, padding='same')\n",
    "            n = bn(nn,training = self.train )\n",
    "            n = tf.add(n,temp)\n",
    "            # B residual blacks end\n",
    "            n = conv2d(n, 256, kernel_size=3, strides=1, padding='same')\n",
    "            n = relu(deconv2d(n, [9, 128, 128 , 256], name='g_h1'))\n",
    "            n = conv2d(n, 256, kernel_size=3, strides=1, padding='same')\n",
    "            n = relu(deconv2d(n, [9, 256, 256 , 256], name='g_h2'))\n",
    "            n = tanh(conv2d(n, 3, kernel_size=3, strides=1, padding='same'))\n",
    "            self.outputs = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet structure\n",
    "class Discriminator:\n",
    "    def __init__(self, image, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self): \n",
    "        lrelu = tf.nn.leaky_relu\n",
    "        conv2d = tf.layers.conv2d\n",
    "        bn = tf.layers.batch_normalization\n",
    "        dense = tf.layers.dense\n",
    "        df_dim = 64\n",
    "        with tf.variable_scope('discriminator', reuse=self.reuse):\n",
    "            n = lrelu(conv2d(self.image, df_dim, kernel_size=4, strides=2, padding='same'))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*2 , kernel_size=4, strides=2, padding='same'),training = self.train))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*4 , kernel_size=4, strides=2, padding='same'),training = self.train))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*8 , kernel_size=4, strides=2, padding='same'),training = self.train))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*16 , kernel_size=4, strides=2, padding='same'),training = self.train))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*32 , kernel_size=4, strides=2, padding='same'),training = self.train))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*16 , kernel_size=1, strides=1, padding='same'),training = self.train))\n",
    "            nn = bn(conv2d(n, df_dim*8 , kernel_size=1, strides=1, padding='same'),training = self.train)\n",
    "            n = lrelu(bn(conv2d(nn, df_dim*2 , kernel_size=1, strides=1, padding='same'),training = self.train))\n",
    "            n = lrelu(bn(conv2d(n, df_dim*2 , kernel_size=3, strides=1, padding='same'),training = self.train))\n",
    "            n = bn(conv2d(n, df_dim*8 , kernel_size=3, strides=1, padding='same'),training = self.train)\n",
    "            n = tf.add(n,nn)\n",
    "            n = tf.layers.Flatten()(n)\n",
    "            d_net = dense(n,1)\n",
    "            self.logits = d_net\n",
    "            #self.discriminator_net = d_net\n",
    "            self.outputs =  d_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vgg19_simple_api(rgb, reuse):\n",
    "    \"\"\"\n",
    "    Build the VGG 19 Model\n",
    "    Parameters\n",
    "    -----------\n",
    "    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]\n",
    "    \"\"\"\n",
    "    VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "    with tf.variable_scope(\"VGG19\", reuse=reuse) as vs:\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "        rgb_scaled = rgb * 255.0\n",
    "        # Convert RGB to BGR\n",
    "        if tf.__version__ <= '0.11':\n",
    "            red, green, blue = tf.split(3, 3, rgb_scaled)\n",
    "        else: # TF 1.0\n",
    "            # print(rgb_scaled)\n",
    "            red, green, blue = tf.split(rgb_scaled, 3, 3)\n",
    "        assert red.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        if tf.__version__ <= '0.11':\n",
    "            bgr = tf.concat(3, [\n",
    "                blue - VGG_MEAN[0],\n",
    "                green - VGG_MEAN[1],\n",
    "                red - VGG_MEAN[2],\n",
    "            ])\n",
    "        else:\n",
    "            bgr = tf.concat([\n",
    "                blue - VGG_MEAN[0],\n",
    "                green - VGG_MEAN[1],\n",
    "                red - VGG_MEAN[2],\n",
    "            ], axis=3)\n",
    "        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "\n",
    "        \"\"\" input layer \"\"\"\n",
    "        net_in = InputLayer(bgr, name='input')\n",
    "        \"\"\" conv1 \"\"\"\n",
    "        network = Conv2d(net_in, n_filter=64, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv1_1')\n",
    "        network = Conv2d(network, n_filter=64, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv1_2')\n",
    "        network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),\n",
    "                    padding='SAME', name='pool1')\n",
    "        \"\"\" conv2 \"\"\"\n",
    "        network = Conv2d(network, n_filter=128, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv2_1')\n",
    "        network = Conv2d(network, n_filter=128, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv2_2')\n",
    "        network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),\n",
    "                    padding='SAME', name='pool2')\n",
    "        \"\"\" conv3 \"\"\"\n",
    "        network = Conv2d(network, n_filter=256, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv3_1')\n",
    "        network = Conv2d(network, n_filter=256, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv3_2')\n",
    "        network = Conv2d(network, n_filter=256, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv3_3')\n",
    "        network = Conv2d(network, n_filter=256, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv3_4')\n",
    "        network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),\n",
    "                    padding='SAME', name='pool3')\n",
    "        \"\"\" conv4 \"\"\"\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv4_1')\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv4_2')\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv4_3')\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv4_4')\n",
    "        network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),\n",
    "                    padding='SAME', name='pool4')                               # (batch_size, 14, 14, 512)\n",
    "        conv = network\n",
    "        \"\"\" conv5 \"\"\"\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv5_1')\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv5_2')\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv5_3')\n",
    "        network = Conv2d(network, n_filter=512, filter_size=(3, 3),\n",
    "                    strides=(1, 1), act=tf.nn.relu,padding='SAME', name='conv5_4')\n",
    "        network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),\n",
    "                    padding='SAME', name='pool5')                               # (batch_size, 7, 7, 512)\n",
    "        \"\"\" fc 6~8 \"\"\"\n",
    "        network = FlattenLayer(network, name='flatten')\n",
    "        network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name='fc6')\n",
    "        network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name='fc7')\n",
    "        network = DenseLayer(network, n_units=1000, act=tf.identity, name='fc8')\n",
    "        print(\"build model finished: %fs\" % (time.time() - start_time))\n",
    "        return network, conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparas():\n",
    "    hparas = {\n",
    "        'BATCH_SIZE' : 9,\n",
    "        'LR' : 1e-4,\n",
    "        'BETA' : 0.9, # AdamOptimizer parameter\n",
    "        'N_EPOCH' : 100,\n",
    "        'N_SAMPLE' : 7480\n",
    "    }\n",
    "    return hparas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, hparas, training_phase, ckpt_path, inference_path, test_img = None,recover=None,savename = None):\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.sample_path = './samples'\n",
    "        self.inference_path = './inference'\n",
    "        \n",
    "        self._get_session() # get session\n",
    "        self._get_train_data_iter() # initialize and get data iterator\n",
    "        self._input_layer() # define input placeholder\n",
    "        self._get_inference() # build generator and discriminator\n",
    "        self._get_loss() # define gan loss\n",
    "        self._get_var_with_name() # get variables for each part of model\n",
    "        self._optimize() # define optimizer\n",
    "        self._init_vars()\n",
    "        self._get_saver()\n",
    "        self.test_LRbatch = test_img\n",
    "        self.savename = savename\n",
    "        if recover is not None:\n",
    "            self._load_checkpoint(recover)\n",
    "            #self.test_LRbatch = test_img\n",
    "            \n",
    "            \n",
    "        #print (self.generator  )  \n",
    "        \n",
    "    def _get_train_data_iter(self):\n",
    "        if self.train: # training data iteratot\n",
    "            iterator_oneshot = train_ds.make_one_shot_iterator()\n",
    "            self.next_element_oneshot = iterator_oneshot.get_next()\n",
    "           \n",
    "            \n",
    "    def _input_layer(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.placeholder(tf.float32,[None, 256,256, 3],name='real_image')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [None, 64,64,3], name='z_noise')\n",
    "        else:\n",
    "            \n",
    "            self.z_noise = tf.placeholder(tf.float32, [None, 64,64,3], name='z_noise')\n",
    "    \n",
    "    def _get_inference(self):\n",
    "        if self.train:\n",
    "            \n",
    "            # generating image\n",
    "            generator = Generator(self.z_noise, training_phase=True,hparas=self.hparas, reuse=False)\n",
    "            self.generator = generator\n",
    "            # discriminize\n",
    "            # fake image\n",
    "            fake_discriminator = Discriminator(generator.outputs,training_phase=True, hparas=self.hparas, reuse=False)\n",
    "            self.fake_discriminator = fake_discriminator\n",
    "            # real image\n",
    "            real_discriminator = Discriminator(self.real_image, training_phase=True,hparas=self.hparas, reuse=True)\n",
    "            self.real_discriminator = real_discriminator\n",
    "            '''VGG'''\n",
    "            t_target_image_224 = tf.image.resize_images(self.real_image, size=[224, 224], method=0, align_corners=False)\n",
    "            t_predict_image_224 = tf.image.resize_images(generator.outputs, size=[224, 224], method=0, align_corners=False)\n",
    "            self.net_vgg, self.vgg_target_emb = Vgg19_simple_api((t_target_image_224+1)/2, reuse=False)\n",
    "            _, self.vgg_predict_emb = Vgg19_simple_api((t_predict_image_224+1)/2, reuse=True)\n",
    "\n",
    "            \n",
    "        else: # inference mode\n",
    "            self.generate_image_net = Generator(self.z_noise, training_phase=True,hparas=self.hparas, reuse=False)\n",
    "            \n",
    "    def _get_loss(self):\n",
    "        if self.train:\n",
    "            d_loss1 = tl.cost.sigmoid_cross_entropy(self.real_discriminator.logits, tf.ones_like(self.real_discriminator.logits), name='d1')\n",
    "            d_loss2 = tl.cost.sigmoid_cross_entropy(self.fake_discriminator.logits, tf.zeros_like(self.fake_discriminator.logits), name='d2')\n",
    "            self.d_loss = d_loss1 + d_loss2\n",
    "            mse_loss = tl.cost.mean_squared_error(self.generator.outputs, self.real_image, is_mean=True)\n",
    "            self.initmse_loss = mse_loss\n",
    "            g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(self.fake_discriminator.logits, tf.ones_like(self.fake_discriminator.logits), name='g')\n",
    "            vgg_loss = 2e-6 * tl.cost.mean_squared_error(self.vgg_predict_emb.outputs, self.vgg_target_emb.outputs, is_mean=True)\n",
    "            self.g_loss = mse_loss + g_gan_loss + vgg_loss\n",
    "    \n",
    "    def _optimize(self):\n",
    "        if self.train:\n",
    "            with tf.variable_scope('learning_rate'):\n",
    "                self.lr_var = tf.Variable(self.hparas['LR'], trainable=False)\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            generator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            g_optim_init_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            \n",
    "            self.d_optim = discriminator_optimizer.minimize(self.d_loss, var_list=self.discrim_vars)\n",
    "            self.g_optim = generator_optimizer.minimize(self.g_loss, var_list=self.generator_vars)\n",
    "            self.g_optim_init = g_optim_init_optimizer.minimize(self.initmse_loss, var_list=self.generator_vars)\n",
    "        \n",
    "    def training(self):\n",
    "        '''VGG'''\n",
    "        vgg19_npy_path = \"./vgg19.npy\"\n",
    "        if not os.path.isfile(vgg19_npy_path):\n",
    "            print(\"Please download vgg19.npz from : https://github.com/machrisaa/tensorflow-vgg\")\n",
    "            exit()\n",
    "        npz = np.load(vgg19_npy_path, encoding='latin1').item()\n",
    "\n",
    "        params = []\n",
    "        for val in sorted( npz.items() ):\n",
    "            W = np.asarray(val[1][0])\n",
    "            b = np.asarray(val[1][1])\n",
    "            print(\"  Loading %s: %s, %s\" % (val[0], W.shape, b.shape))\n",
    "            params.extend([W, b])\n",
    "        tl.files.assign_params(self.sess, params, self.net_vgg)\n",
    "        # net_vgg.print_params(False)\n",
    "        # net_vgg.print_layers()\n",
    "\n",
    "        for _epoch in range(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "            n_batch_epoch = int(self.hparas['N_SAMPLE']/self.hparas['BATCH_SIZE'])\n",
    "            if _epoch < 5:\n",
    "                for _step in range(n_batch_epoch):\n",
    "                    step_time = time.time()\n",
    "                    lr_batch, hr_batch = self.sess.run(self.next_element_oneshot)\n",
    "                    self.lr_batch = lr_batch\n",
    "                    self.hr_batch = hr_batch\n",
    "                    self.generator_error, _ = self.sess.run([self.initmse_loss, self.g_optim_init],\n",
    "                                                           feed_dict={self.z_noise:self.lr_batch ,\n",
    "                                                                      self.real_image:self.hr_batch})\n",
    "                    if _step%50==0:\n",
    "                        print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, g_loss: %.3f\" \\\n",
    "                                % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch, time.time() - step_time,\n",
    "                                    self.generator_error))\n",
    "            else:\n",
    "                for _step in range(n_batch_epoch):\n",
    "                    step_time = time.time()\n",
    "                    lr_batch, hr_batch = self.sess.run(self.next_element_oneshot)\n",
    "                    self.lr_batch = lr_batch\n",
    "                    self.hr_batch = hr_batch\n",
    "                    # update discriminator\n",
    "                    self.discriminator_error, _ = self.sess.run([self.d_loss, self.d_optim],\n",
    "                                                                   feed_dict={\n",
    "                                                                        self.real_image:self.hr_batch,\n",
    "                                                                        self.z_noise:self.lr_batch})\n",
    "\n",
    "                    # update generate\n",
    "                    self.generator_error, _ = self.sess.run([self.g_loss, self.g_optim],\n",
    "                                                           feed_dict={self.z_noise:self.lr_batch ,\n",
    "                                                                      self.real_image:self.hr_batch})\n",
    "\n",
    "\n",
    "                    if _step%50==0:\n",
    "                        print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.3f, g_loss: %.3f\" \\\n",
    "                                % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch, time.time() - step_time,\n",
    "                                   self.discriminator_error, self.generator_error))\n",
    "                    \n",
    "                self._save_checkpoint(_epoch)\n",
    "            self._sample_visiualize(_epoch)\n",
    "            #self._sample_visiualize(_epoch)\n",
    "            \n",
    "    def inference(self):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        img_gen = self.sess.run(self.generate_image_net.outputs, feed_dict={self.z_noise : self.test_LRbatch})\n",
    "        save_images(img_gen, [ni, ni], './test_img/SR'+self.savename+'.png')\n",
    "        print('-----success save SRcrop--------')\n",
    "        \n",
    "    def _init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _get_session(self):\n",
    "        self.sess = tf.Session()\n",
    "    \n",
    "    def _get_saver(self):\n",
    "        if self.train:\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.d_saver = tf.train.Saver(var_list=self.discrim_vars)\n",
    "        else:\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            \n",
    "    def _sample_visiualize(self, epoch):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        img_gen = self.sess.run(self.generator.outputs,feed_dict={self.z_noise : self.test_LRbatch})\n",
    "        save_images(img_gen, [ni, ni], './SR_training/Gentrain_{:02d}.png'.format(epoch))\n",
    "        \n",
    "    def _get_var_with_name(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.generator_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        self.discrim_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "    \n",
    "    def _load_checkpoint(self, recover):\n",
    "        if self.train:\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "            self.d_saver.restore(self.sess, self.ckpt_path+'d_model_'+str(recover)+'.ckpt')\n",
    "        else:\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "        print('-----success restored checkpoint--------')\n",
    "    \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        self.g_saver.save(self.sess, self.ckpt_path+'g_model_'+str(epoch)+'.ckpt')\n",
    "        self.d_saver.save(self.sess, self.ckpt_path+'d_model_'+str(epoch)+'.ckpt')\n",
    "        print('-----success saved checkpoint--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "crop_img = Image.open('./000057_3.png')\n",
    "#plt.imshow(crop_img)\n",
    "testLR_img = []\n",
    "width, height = crop_img.size\n",
    "start_pos = start_x, start_y = (0, 0)\n",
    "cropped_image_size = w, h = (64, 64)\n",
    "\n",
    "for row_i in range(0, width, w):\n",
    "    for col_i in range(0, height, h):\n",
    "        lr = crop_img.crop((col_i, row_i, col_i + w, row_i + h))\n",
    "        pic = np.array(lr)\n",
    "        testLR_img.append(pic)\n",
    "LRimgs = np.array(testLR_img)\n",
    "print(LRimgs.shape)\n",
    "LRimgs = LRimgs / (255. / 2.)\n",
    "LRimgs = LRimgs - 1.\n",
    "#print(LRimgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model started\n",
      "[TL] InputLayer  VGG19/input: (?, 224, 224, 3)\n",
      "[TL] Conv2d VGG19/conv1_1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv1_2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool1: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv2_1: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv2_2: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool2: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv3_1: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv3_2: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv3_3: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv3_4: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool3: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv4_1: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv4_2: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv4_3: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv4_4: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool4: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv5_1: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv5_2: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv5_3: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv5_4: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool5: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] FlattenLayer VGG19/flatten: 25088\n",
      "[TL] DenseLayer  VGG19/fc6: 4096 relu\n",
      "[TL] DenseLayer  VGG19/fc7: 4096 relu\n",
      "[TL] DenseLayer  VGG19/fc8: 1000 No Activation\n",
      "build model finished: 11.450474s\n",
      "build model started\n",
      "[TL] InputLayer  VGG19/input: (9, 224, 224, 3)\n",
      "[TL] Conv2d VGG19/conv1_1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv1_2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool1: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv2_1: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv2_2: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool2: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv3_1: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv3_2: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv3_3: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv3_4: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool3: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv4_1: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv4_2: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv4_3: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv4_4: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool4: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d VGG19/conv5_1: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv5_2: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv5_3: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d VGG19/conv5_4: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d VGG19/pool5: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] FlattenLayer VGG19/flatten: 25088\n",
      "[TL] DenseLayer  VGG19/fc6: 4096 relu\n",
      "[TL] DenseLayer  VGG19/fc7: 4096 relu\n",
      "[TL] DenseLayer  VGG19/fc8: 1000 No Activation\n",
      "build model finished: 10.726557s\n",
      "  Loading conv1_1: (3, 3, 3, 64), (64,)\n",
      "  Loading conv1_2: (3, 3, 64, 64), (64,)\n",
      "  Loading conv2_1: (3, 3, 64, 128), (128,)\n",
      "  Loading conv2_2: (3, 3, 128, 128), (128,)\n",
      "  Loading conv3_1: (3, 3, 128, 256), (256,)\n",
      "  Loading conv3_2: (3, 3, 256, 256), (256,)\n",
      "  Loading conv3_3: (3, 3, 256, 256), (256,)\n",
      "  Loading conv3_4: (3, 3, 256, 256), (256,)\n",
      "  Loading conv4_1: (3, 3, 256, 512), (512,)\n",
      "  Loading conv4_2: (3, 3, 512, 512), (512,)\n",
      "  Loading conv4_3: (3, 3, 512, 512), (512,)\n",
      "  Loading conv4_4: (3, 3, 512, 512), (512,)\n",
      "  Loading conv5_1: (3, 3, 512, 512), (512,)\n",
      "  Loading conv5_2: (3, 3, 512, 512), (512,)\n",
      "  Loading conv5_3: (3, 3, 512, 512), (512,)\n",
      "  Loading conv5_4: (3, 3, 512, 512), (512,)\n",
      "  Loading fc6: (25088, 4096), (4096,)\n",
      "  Loading fc7: (4096, 4096), (4096,)\n",
      "  Loading fc8: (4096, 1000), (1000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'\n",
    "gan = GAN(get_hparas(), training_phase=True, ckpt_path=checkpoint_path, inference_path=inference_path,test_img = LRimgs)\n",
    "gan.training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/g_model_60.ckpt\n",
      "-----success restored checkpoint--------\n",
      "-----success save SRcrop--------\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'\n",
    "imgname = \"000057_3\"\n",
    "tf.reset_default_graph()\n",
    "gan = GAN(get_hparas(), training_phase=False, ckpt_path=checkpoint_path, inference_path=inference_path,test_img = LRimgs ,recover=60,savename = imgname)\n",
    "img = gan.inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
